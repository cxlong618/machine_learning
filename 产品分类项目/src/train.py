#!/usr/bin/env python3
"""
æœ€ç»ˆè®­ç»ƒè„šæœ¬ - äº§å“åˆ†ç±»å¤šä»»åŠ¡BERTæ¨¡å‹
å®Œå…¨ä½¿ç”¨ModelScopeï¼Œæ— HuggingFaceä¾èµ–
"""
import os
import sys
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def check_data_files(train_path=None, val_path=None):
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    logger.info("æ£€æŸ¥æ•°æ®æ–‡ä»¶...")

    if train_path and val_path:
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        required_files = [train_path, val_path]
    else:
        # ä½¿ç”¨é»˜è®¤è·¯å¾„
        required_files = ['data/train.csv', 'data/val.csv']

    missing_files = [f for f in required_files if not os.path.exists(f)]

    if missing_files:
        logger.error(f"ç¼ºå¤±æ•°æ®æ–‡ä»¶: {missing_files}")
        logger.info("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨:")
        for f in missing_files:
            logger.info(f"   - {f}")
        return False

    logger.info("æ•°æ®æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='äº§å“åˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--train_path', type=str, required=True, help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--val_path', type=str, required=True, help='éªŒè¯æ•°æ®CSVæ–‡ä»¶è·¯å¾„')

    # å¯é€‰å‚æ•°
    parser.add_argument('--max_length', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 128)')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)')
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='å­¦ä¹ ç‡ (é»˜è®¤: 2e-5)')
    parser.add_argument('--num_epochs', type=int, default=10, help='è®­ç»ƒè½®æ•° (é»˜è®¤: 10)')
    parser.add_argument('--model_name', type=str, default='dienstag/chinese-bert-wwm-ext', help='åŸºç¡€æ¨¡å‹åç§°')
    parser.add_argument('--warmup_steps', type=int, default=500, help='é¢„çƒ­æ­¥æ•° (é»˜è®¤: 500)')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡ (é»˜è®¤: 0.01)')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (é»˜è®¤: 1)')

    return parser.parse_args()


def run_training():
    """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
    args = parse_arguments()

    logger.info("å¯åŠ¨äº§å“åˆ†ç±»æ¨¡å‹è®­ç»ƒ...")
    logger.info("=" * 60)

    # æ˜¾ç¤ºè®­ç»ƒå‚æ•°
    logger.info("è®­ç»ƒå‚æ•°:")
    logger.info(f"  è®­ç»ƒæ•°æ®: {args.train_path}")
    logger.info(f"  éªŒè¯æ•°æ®: {args.val_path}")
    logger.info(f"  æœ€å¤§é•¿åº¦: {args.max_length}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    logger.info(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    logger.info(f"  åŸºç¡€æ¨¡å‹: {args.model_name}")
    logger.info(f"  æƒé‡è¡°å‡: {args.weight_decay}")
    logger.info("=" * 60)

    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader
        from torch.optim import AdamW
        from transformers import get_linear_schedule_with_warmup
        from sklearn.metrics import classification_report, accuracy_score
        import numpy as np
        from datetime import datetime
        import json
        from tqdm import tqdm
        import wandb

        # å¯¼å…¥é¡¹ç›®æ¨¡å—
        from dataset import ProductDataset, DataCollator
        from model import MultiTaskProductClassifier
        from modelscope_utils import load_tokenizer, load_bert_model

        logger.info("æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")

        # æ£€æŸ¥CUDA
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name()}")
            logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'

        # åˆå§‹åŒ–WandBï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
        os.environ['WANDB_MODE'] = 'offline'
        wandb.init(
            project="product-classification",
            name=f"training-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            config=vars(args),
            mode="offline"
        )

        # åŠ è½½åˆ†è¯å™¨
        logger.info("åŠ è½½åˆ†è¯å™¨...")
        tokenizer = load_tokenizer(args.model_name)

        # åˆ›å»ºæ•°æ®é›†
        logger.info("åˆ›å»ºæ•°æ®é›†...")
        train_dataset = ProductDataset(
            data_path=args.train_path,
            tokenizer=tokenizer,
            max_length=args.max_length,
            is_train=True
        )

        val_dataset = ProductDataset(
            data_path=args.val_path,
            tokenizer=tokenizer,
            max_length=args.max_length,
            is_train=False
        )

        logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")

        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        data_collator = DataCollator(tokenizer, max_length=args.max_length)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=0,  # Windowså…¼å®¹
            pin_memory=True if torch.cuda.is_available() else False,
            collate_fn=data_collator
        )

        val_dataloader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False,
            collate_fn=data_collator
        )

        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model_config = {
            'num_labels_standard': len(train_dataset.standard_mapping),
            'num_labels_level1': len(train_dataset.level1_mapping),
            'num_labels_level2': len(train_dataset.level2_mapping),
            'num_labels_level3': len(train_dataset.level3_mapping),
            'loss_weights': {'standard': 0.4, 'level1': 0.2, 'level2': 0.2, 'level3': 0.2}
        }

        model = MultiTaskProductClassifier.from_pretrained(
            args.model_name,
            **model_config
        )
        model.to(device)

        logger.info(f"æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}")

        # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = AdamW(
            model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )

        total_steps = len(train_dataloader) * args.num_epochs // args.gradient_accumulation_steps
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=total_steps
        )

        logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")

        # è®­ç»ƒå¾ªç¯
        logger.info("å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 3

        for epoch in range(args.num_epochs):
            logger.info(f"Epoch {epoch + 1}/{args.num_epochs}")

            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_train_loss = 0
            train_progress = tqdm(train_dataloader, desc=f"è®­ç»ƒ Epoch {epoch + 1}")

            for step, batch in enumerate(train_progress):
                # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(device) for k, v in batch.items()}

                outputs = model(**batch)
                loss = outputs['loss']

                # æ¢¯åº¦ç´¯ç§¯
                if args.gradient_accumulation_steps > 1:
                    loss = loss / args.gradient_accumulation_steps

                loss.backward()
                total_train_loss += loss.item()

                if (step + 1) % args.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()

                # æ›´æ–°è¿›åº¦æ¡
                train_progress.set_postfix({'loss': loss.item()})

                # è®°å½•åˆ°WandB
                if step % 100 == 0:
                    wandb.log({
                        'train_loss': loss.item(),
                        'learning_rate': scheduler.get_last_lr()[0],
                        'step': epoch * len(train_dataloader) + step
                    })

            avg_train_loss = total_train_loss / len(train_dataloader)
            logger.info(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")

            # éªŒè¯é˜¶æ®µ
            model.eval()
            total_val_loss = 0
            all_predictions = {'standard': [], 'level1': [], 'level2': [], 'level3': []}
            all_labels = {'standard': [], 'level1': [], 'level2': [], 'level3': []}

            with torch.no_grad():
                val_progress = tqdm(val_dataloader, desc=f"éªŒè¯ Epoch {epoch + 1}")

                for batch in val_progress:
                    # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    batch = {k: v.to(device) for k, v in batch.items()}

                    outputs = model(**batch)
                    loss = outputs['loss']
                    total_val_loss += loss.item()

                    # æ”¶é›†é¢„æµ‹ç»“æœ - å…¼å®¹æ€§ä¿®å¤
                    for task in ['standard', 'level1', 'level2', 'level3']:
                        preds = torch.argmax(outputs[f'logits_{task}'], dim=-1)
                        labels = batch[f'labels_{task}']

                        # å…¼å®¹æ€§numpyè½¬æ¢
                        try:
                            all_predictions[task].extend(preds.cpu().numpy())
                            all_labels[task].extend(labels.cpu().numpy())
                        except RuntimeError as e:
                            if "Numpy is not available" in str(e):
                                # ä½¿ç”¨PyTorchåŸç”Ÿæ–¹æ³•
                                all_predictions[task].extend(preds.cpu().tolist())
                                all_labels[task].extend(labels.cpu().tolist())
                            else:
                                raise

                    val_progress.set_postfix({'val_loss': loss.item()})

            avg_val_loss = total_val_loss / len(val_dataloader)
            logger.info(f"éªŒè¯æŸå¤±: {avg_val_loss:.4f}")

            # è®¡ç®—å‡†ç¡®ç‡ - å…¼å®¹æ€§ä¿®å¤
            val_accuracies = {}
            for task in ['standard', 'level1', 'level2', 'level3']:
                # è½¬æ¢ä¸ºnumpyæ•°ç»„å…¼å®¹æ ¼å¼
                try:
                    labels_array = np.array(all_labels[task])
                    preds_array = np.array(all_predictions[task])
                    accuracy = accuracy_score(labels_array, preds_array)
                except (RuntimeError, ImportError):
                    # å¦‚æœnumpyä¸å¯ç”¨ï¼Œæ‰‹åŠ¨è®¡ç®—å‡†ç¡®ç‡
                    correct = sum(1 for l, p in zip(all_labels[task], all_predictions[task]) if l == p)
                    accuracy = correct / len(all_labels[task]) if all_labels[task] else 0

                val_accuracies[task] = accuracy
                logger.info(f"{task} å‡†ç¡®ç‡: {accuracy:.4f}")

            # è®°å½•åˆ°WandB - å…¼å®¹æ€§ä¿®å¤
            accuracy_values = list(val_accuracies.values())
            avg_accuracy = sum(accuracy_values) / len(accuracy_values) if accuracy_values else 0

            wandb.log({
                'epoch': epoch + 1,
                'avg_train_loss': avg_train_loss,
                'avg_val_loss': avg_val_loss,
                'val_accuracy_standard': val_accuracies['standard'],
                'val_accuracy_level1': val_accuracies['level1'],
                'val_accuracy_level2': val_accuracies['level2'],
                'val_accuracy_level3': val_accuracies['level3'],
                'val_accuracy_avg': avg_accuracy
            })

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0

                # ç¡®ä¿modelsç›®å½•å­˜åœ¨
                os.makedirs('../models', exist_ok=True)

                # ä¿å­˜æ¨¡å‹
                model.save_pretrained('../models/best_model')
                tokenizer.save_pretrained('../models/tokenizer')

                # ä¿å­˜æ ‡ç­¾æ˜ å°„
                label_mappings = {
                    'standard_name': train_dataset.standard_mapping,
                    'level1_category': train_dataset.level1_mapping,
                    'level2_category': train_dataset.level2_mapping,
                    'level3_category': train_dataset.level3_mapping
                }

                with open('../models/label_mappings.json', 'w', encoding='utf-8') as f:
                    json.dump(label_mappings, f, ensure_ascii=False, indent=2)

                logger.info("âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                patience_counter += 1
                logger.info(f"éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{max_patience}")

                if patience_counter >= max_patience:
                    logger.info("æ—©åœè®­ç»ƒ")
                    break

        # è®­ç»ƒå®Œæˆ
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        wandb.finish()

        return True

    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    try:
        args = parse_arguments()
    except SystemExit:
        # å‚æ•°è§£æå¤±è´¥ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        return False

    print("äº§å“åˆ†ç±»æ¨¡å‹ - ModelScopeæœ€ç»ˆç‰ˆ")
    print("=" * 60)

    # æ˜¾ç¤ºModelScopeä¿¡æ¯
    print("ModelScope é…ç½®ä¿¡æ¯:")
    print("  - æ¨¡å‹æº: ModelScope (é­”æ­)")
    print("  - æ¨¡å‹åç§°: dienstag/chinese-bert-wwm-ext")
    print("  - ä¸‹è½½å¹³å°: å›½å†…é«˜é€ŸæœåŠ¡å™¨")
    print("  - æ— HuggingFaceä¾èµ–: OK")
    print("  - è‡ªåŠ¨ç¼“å­˜: OK")
    print("  - ç¦»çº¿è¿è¡Œ: OK")

    # æ•°æ®æ–‡ä»¶æ£€æŸ¥
    if not check_data_files(args.train_path, args.val_path):
        logger.error("æ•°æ®æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
        return False

    # å¼€å§‹è®­ç»ƒ
    success = run_training()

    if success:
        print("\nè®­ç»ƒå®Œæˆ!")
        print("è¾“å‡ºæ–‡ä»¶:")
        print("  - models/best_model/: æœ€ä½³æ¨¡å‹æ–‡ä»¶")
        print("  - models/tokenizer/: åˆ†è¯å™¨æ–‡ä»¶")
        print("  - models/label_mappings.json: æ ‡ç­¾æ˜ å°„æ–‡ä»¶")
        print("  - logs/training.log: è®­ç»ƒæ—¥å¿—")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. ä¸‹è½½ models/ ç›®å½•åˆ°æœ¬åœ°")
        print("  2. è¿è¡Œ python run_inference.py æµ‹è¯•æ¨ç†")
        print("  3. è¿è¡Œ python deploy_app.py å¯åŠ¨WebæœåŠ¡")
        print("\nç›‘æ§å’Œæ—¥å¿—:")
        print("  - WandBé¡¹ç›®é¡µé¢æŸ¥çœ‹è¯¦ç»†æŒ‡æ ‡")
        print("  - æœ¬åœ°æ—¥å¿—: tail -f logs/training.log")
    else:
        print("\nè®­ç»ƒå¤±è´¥")
        print("è¯·æ£€æŸ¥:")
        print("  1. æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("  2. Pythonç¯å¢ƒå’Œä¾èµ–åŒ…")
        print("  3. ç½‘ç»œè¿æ¥çŠ¶æ€")
        print("  4. GPUèµ„æºæ˜¯å¦å¯ç”¨")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)