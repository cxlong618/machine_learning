# æ·±åº¦å­¦ä¹ åŸºç¡€

## ğŸ¯ æ·±åº¦å­¦ä¹ æ¦‚è¿°

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å…·æœ‰å¤šå±‚ç»“æ„çš„ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚

### ç¥ç»å…ƒå’Œç¥ç»ç½‘ç»œ

**ç”Ÿç‰©ç¥ç»å…ƒç±»æ¯”**ï¼š
- **æ ‘çª**ï¼šæ¥æ”¶è¾“å…¥ä¿¡å·
- **ç»†èƒä½“**ï¼šå¤„ç†ä¿¡æ¯
- **è½´çª**ï¼šä¼ é€’è¾“å‡ºä¿¡å·

**äººå·¥ç¥ç»å…ƒæ¨¡å‹**ï¼š
```
è¾“å‡º = æ¿€æ´»å‡½æ•°(æƒé‡ Ã— è¾“å…¥ + åç½®)
```

## ğŸ§  æ ¸å¿ƒæ¦‚å¿µ

### 1. æ¿€æ´»å‡½æ•°

**Sigmoidå‡½æ•°**ï¼š
```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
- èŒƒå›´ï¼š(0, 1)
- ä¼˜ç‚¹ï¼šå¹³æ»‘è¾“å‡º
- ç¼ºç‚¹ï¼šæ¢¯åº¦æ¶ˆå¤±

**ReLUå‡½æ•°**ï¼š
```python
def relu(x):
    return np.maximum(0, x)
```
- èŒƒå›´ï¼š[0, âˆ)
- ä¼˜ç‚¹ï¼šè®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- ç¼ºç‚¹ï¼šç¥ç»å…ƒæ­»äº¡é—®é¢˜

**Leaky ReLU**ï¼š
```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)
```

**Softmaxå‡½æ•°**ï¼š
```python
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # é˜²æ­¢æº¢å‡º
    return exp_x / np.sum(exp_x)
```
- å¤šåˆ†ç±»è¾“å‡ºå±‚
- è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ

### 2. æŸå¤±å‡½æ•°

**å‡æ–¹è¯¯å·® (MSE)**ï¼š
```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
```
- å›å½’é—®é¢˜

**äº¤å‰ç†µæŸå¤±**ï¼š
```python
def cross_entropy_loss(y_true, y_pred):
    # é˜²æ­¢log(0)
    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
    return -np.sum(y_true * np.log(y_pred))
```
- åˆ†ç±»é—®é¢˜

### 3. åå‘ä¼ æ’­

**é“¾å¼æ³•åˆ™**ï¼š
```
âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z Ã— âˆ‚z/âˆ‚w
```

**æ¢¯åº¦è®¡ç®—ç¤ºä¾‹**ï¼š
```python
def backward_pass(X, y, weights, activations):
    m = X.shape[0]

    # è¾“å‡ºå±‚æ¢¯åº¦
    dz3 = activations[2] - y
    dw3 = (1/m) * np.dot(activations[1].T, dz3)
    db3 = (1/m) * np.sum(dz3, axis=0, keepdims=True)

    # éšè—å±‚æ¢¯åº¦
    da2 = np.dot(dz3, weights[2].T)
    dz2 = da2 * relu_derivative(activations[1])
    dw2 = (1/m) * np.dot(activations[0].T, dz2)
    db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)

    return [dw2, dw3], [db2, db3]
```

## ğŸ—ï¸ ç½‘ç»œæ¶æ„

### 1. å‰é¦ˆç¥ç»ç½‘ç»œ (FNN)

**ç»“æ„**ï¼š
- è¾“å…¥å±‚
- éšè—å±‚
- è¾“å‡ºå±‚

**å‰å‘ä¼ æ’­**ï¼š
```python
def forward_pass(X, weights, biases):
    activations = []
    z = np.dot(X, weights[0]) + biases[0]
    a = relu(z)
    activations.append(a)

    z = np.dot(a, weights[1]) + biases[1]
    a = sigmoid(z)
    activations.append(a)

    return activations
```

### 2. å·ç§¯ç¥ç»ç½‘ç»œ (CNN)

**æ ¸å¿ƒç»„ä»¶**ï¼š

**å·ç§¯å±‚**ï¼š
```python
import torch
import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # è¾“å…¥3é€šé“ï¼Œè¾“å‡º32é€šé“ï¼Œ3x3å·ç§¯
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)  # 2x2æœ€å¤§æ± åŒ–

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        return x
```

**æ± åŒ–å±‚**ï¼š
- æœ€å¤§æ± åŒ–ï¼šä¿ç•™æœ€æ˜¾è‘—ç‰¹å¾
- å¹³å‡æ± åŒ–ï¼šä¿ç•™æ•´ä½“ä¿¡æ¯

**Batch Normalization**ï¼š
```python
self.bn = nn.BatchNorm2d(64)
```

### 3. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)

**åŸºæœ¬RNN**ï¼š
```python
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        output, hidden = self.rnn(x)
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.fc(output[:, -1, :])
        return output
```

**LSTM (é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)**ï¼š
```python
class LSTMNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        output, (hidden, cell) = self.lstm(x)
        output = self.fc(output[:, -1, :])
        return output
```

**GRU (é—¨æ§å¾ªç¯å•å…ƒ)**ï¼š
```python
self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
```

### 4. Transformer

**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼š
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.d_k = d_model // num_heads

        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)

    def forward(self, query, key, value):
        # å¤šå¤´æ³¨æ„åŠ›å®ç°
        batch_size = query.size(0)

        # çº¿æ€§å˜æ¢
        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        attention = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)

        return self.w_o(output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model))
```

## ğŸ”§ è®­ç»ƒæŠ€å·§

### 1. ä¼˜åŒ–ç®—æ³•

**SGD (éšæœºæ¢¯åº¦ä¸‹é™)**ï¼š
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

**Adamä¼˜åŒ–å™¨**ï¼š
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

**å­¦ä¹ ç‡è°ƒåº¦**ï¼š
```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')
```

### 2. æ­£åˆ™åŒ–æŠ€æœ¯

**Dropout**ï¼š
```python
self.dropout = nn.Dropout(0.5)  # 50%çš„ç¥ç»å…ƒè¢«ä¸¢å¼ƒ
x = self.dropout(x)
```

**L2æ­£åˆ™åŒ–**ï¼š
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
```

**æ—©åœ**ï¼š
```python
class EarlyStopping:
    def __init__(self, patience=7, verbose=False):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score:
            self.counter += 1
            if self.counter >= self.patience:
                return True
        else:
            self.best_score = score
            self.counter = 0
        return False
```

### 3. æ•°æ®å¢å¼º

**å›¾åƒå¢å¼º**ï¼š
```python
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.RandomResizedCrop(224),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

## ğŸ“Š å®æˆ˜é¡¹ç›®

### 1. å›¾åƒåˆ†ç±»

```python
import torch
import torch.nn as nn
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# æ•°æ®åŠ è½½
transform = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# æ¨¡å‹å®šä¹‰
class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),

            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )

        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# è®­ç»ƒå¾ªç¯
def train_model(model, train_loader, criterion, optimizer, epochs):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, '
              f'Accuracy: {100.*correct/total:.2f}%')

# ä½¿ç”¨ç¤ºä¾‹
model = CNN(num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
train_model(model, train_loader, criterion, optimizer, epochs=50)
```

### 2. æ–‡æœ¬åˆ†ç±»

```python
import torch
import torch.nn as nn
from torchtext.datasets import IMDB
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

tokenizer = get_tokenizer('basic_english')

# æ„å»ºè¯æ±‡è¡¨
def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(IMDB(split='train')), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

# æ–‡æœ¬å¤„ç†
def text_pipeline(text):
    return vocab(tokenizer(text))

class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, _) = self.lstm(embedded)
        return self.fc(hidden.squeeze(0))
```

## âš ï¸ å¸¸è§é—®é¢˜

### 1. æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸
- **è§£å†³æ–¹æ¡ˆ**ï¼šæ¢¯åº¦è£å‰ªã€æ®‹å·®è¿æ¥ã€æ‰¹å½’ä¸€åŒ–

### 2. è¿‡æ‹Ÿåˆ
- **è§£å†³æ–¹æ¡ˆ**ï¼šDropoutã€æ­£åˆ™åŒ–ã€æ•°æ®å¢å¼ºã€æ—©åœ

### 3. è®­ç»ƒä¸ç¨³å®š
- **è§£å†³æ–¹æ¡ˆ**ï¼šåˆé€‚çš„åˆå§‹åŒ–ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ‰¹å½’ä¸€åŒ–

## ğŸš€ å‘å±•è¶‹åŠ¿

### 1. é¢„è®­ç»ƒæ¨¡å‹
- BERT, GPTç³»åˆ—
- è¿ç§»å­¦ä¹ 
- Fine-tuning

### 2. è‡ªç›‘ç£å­¦ä¹ 
- å¯¹æ¯”å­¦ä¹ 
- æ©ç è¯­è¨€æ¨¡å‹
- æ— æ ‡ç­¾æ•°æ®åˆ©ç”¨

### 3. æ¨¡å‹å‹ç¼©
- çŸ¥è¯†è’¸é¦
- å‰ªæ
- é‡åŒ–

---

*æ·±åº¦å­¦ä¹ æ˜¯å½“å‰AIæœ€æ´»è·ƒçš„é¢†åŸŸï¼ŒæŒç»­å­¦ä¹ å’Œå®è·µæ˜¯æŒæ¡å®ƒçš„å…³é”®ï¼*