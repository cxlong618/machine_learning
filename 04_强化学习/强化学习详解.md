# å¼ºåŒ–å­¦ä¹ è¯¦è§£

## ğŸ¯ å¼ºåŒ–å­¦ä¹ æ¦‚è¿°

å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡æ™ºèƒ½ä½“ (Agent) ä¸ç¯å¢ƒ (Environment) çš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

### æ ¸å¿ƒæ€æƒ³
> é€šè¿‡è¯•é”™å­¦ä¹ ï¼Œåœ¨æ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±

## ğŸ§© åŸºæœ¬æ¦‚å¿µ

### 1. æ™ºèƒ½ä½“ (Agent)
- **å®šä¹‰**ï¼šèƒ½å¤Ÿè§‚å¯Ÿç¯å¢ƒå¹¶é‡‡å–è¡ŒåŠ¨çš„å­¦ä¹ è€…
- **ç‰¹ç‚¹**ï¼šå…·æœ‰æ„ŸçŸ¥ã€å†³ç­–ã€å­¦ä¹ èƒ½åŠ›

### 2. ç¯å¢ƒ (Environment)
- **å®šä¹‰**ï¼šæ™ºèƒ½ä½“æ‰€å¤„çš„å¤–éƒ¨ä¸–ç•Œ
- **ç‰¹ç‚¹**ï¼šå“åº”æ™ºèƒ½ä½“è¡ŒåŠ¨ï¼Œæä¾›å¥–åŠ±å’Œæ–°çš„çŠ¶æ€

### 3. çŠ¶æ€ (State, S)
- **å®šä¹‰**ï¼šå¯¹ç¯å¢ƒçš„æè¿°
- **ç±»å‹**ï¼š
  - ç¦»æ•£çŠ¶æ€ï¼šæœ‰é™æ•°é‡çš„çŠ¶æ€
  - è¿ç»­çŠ¶æ€ï¼šæ— é™çŠ¶æ€ç©ºé—´

### 4. åŠ¨ä½œ (Action, A)
- **å®šä¹‰**ï¼šæ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ
- **ç±»å‹**ï¼š
  - ç¦»æ•£åŠ¨ä½œï¼šæœ‰é™çš„åŠ¨ä½œé›†åˆ
  - è¿ç»­åŠ¨ä½œï¼šè¿ç»­çš„åŠ¨ä½œç©ºé—´

### 5. å¥–åŠ± (Reward, R)
- **å®šä¹‰**ï¼šå¯¹æ™ºèƒ½ä½“è¡ŒåŠ¨çš„å³æ—¶åé¦ˆ
- **ä½œç”¨**ï¼šæŒ‡å¯¼å­¦ä¹ æ–¹å‘
- **è®¾è®¡åŸåˆ™**ï¼š
  - æ˜ç¡®è¡¨è¾¾ç›®æ ‡
  - é¿å…å¥–åŠ±ç¨€ç–
  - è€ƒè™‘é•¿æœŸå½±å“

### 6. ç­–ç•¥ (Policy, Ï€)
- **å®šä¹‰**ï¼šä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„
- **è¡¨ç¤º**ï¼šÏ€(a|s) = åœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œaçš„æ¦‚ç‡

### 7. ä»·å€¼å‡½æ•° (Value Function)
- **çŠ¶æ€ä»·å€¼**ï¼šV(s) = ä»çŠ¶æ€så¼€å§‹çš„æœŸæœ›å›æŠ¥
- **åŠ¨ä½œä»·å€¼**ï¼šQ(s,a) = åœ¨çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaçš„æœŸæœ›å›æŠ¥

## ğŸ“Š é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

### å®šä¹‰
é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æ¡†æ¶ï¼ŒåŒ…å«ï¼š
- Sï¼šçŠ¶æ€ç©ºé—´
- Aï¼šåŠ¨ä½œç©ºé—´
- Pï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡
- Rï¼šå¥–åŠ±å‡½æ•°
- Î³ï¼šæŠ˜æ‰£å› å­

### é©¬å°”å¯å¤«æ€§è´¨
> æœªæ¥çš„çŠ¶æ€åªä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä¸å†å²æ— å…³

## ğŸ› ï¸ å¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ†ç±»

### 1. åŸºäºä»·å€¼çš„æ–¹æ³• (Value-based)

**Q-Learning**ï¼š
```
Q(s,a) â† Q(s,a) + Î±[r + Î³maxQ(s',a') - Q(s,a)]
```

**ç‰¹ç‚¹**ï¼š
- å­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•°
- ä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥
- ç¦»æ•£çŠ¶æ€å’ŒåŠ¨ä½œ

```python
import numpy as np

class QLearning:
    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):
        self.q_table = np.zeros((state_space, action_space))
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon

    def choose_action(self, state):
        if np.random.random() < self.epsilon:
            return np.random.choice(self.q_table.shape[1])  # æ¢ç´¢
        else:
            return np.argmax(self.q_table[state])  # åˆ©ç”¨

    def learn(self, state, action, reward, next_state):
        predict = self.q_table[state, action]
        target = reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.lr * (target - predict)
```

**Deep Q-Network (DQN)**ï¼š
- ä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°
- ç»éªŒå›æ”¾ (Experience Replay)
- ç›®æ ‡ç½‘ç»œ (Target Network)

### 2. åŸºäºç­–ç•¥çš„æ–¹æ³• (Policy-based)

**REINFORCEç®—æ³•**ï¼š
```
âˆ‡Î¸ J(Î¸) = E[âˆ‡Î¸ log Ï€(a|s) * G]
```

**ç‰¹ç‚¹**ï¼š
- ç›´æ¥ä¼˜åŒ–ç­–ç•¥
- é€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´
- å¯å¤„ç†éšæœºç­–ç•¥

**Actor-Criticæ–¹æ³•**ï¼š
- Actorï¼šå­¦ä¹ ç­–ç•¥
- Criticï¼šå­¦ä¹ ä»·å€¼å‡½æ•°
- ç»“åˆä¸¤ç§æ–¹æ³•çš„ä¼˜åŠ¿

```python
class ActorCritic:
    def __init__(self, state_dim, action_dim):
        self.actor = PolicyNetwork(state_dim, action_dim)
        self.critic = ValueNetwork(state_dim)

    def select_action(self, state):
        action_probs = self.actor(state)
        return torch.multinomial(action_probs, 1).item()

    def update(self, state, action, reward, next_state):
        # Criticæ›´æ–°
        td_target = reward + self.gamma * self.critic(next_state)
        td_error = td_target - self.critic(state)
        critic_loss = td_error.pow(2)

        # Actoræ›´æ–°
        action_probs = self.actor(state)
        log_prob = torch.log(action_probs[action])
        actor_loss = -log_prob * td_error.detach()

        # åå‘ä¼ æ’­
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
```

### 3. é«˜çº§ç®—æ³•

**PPO (Proximal Policy Optimization)**ï¼š
- è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
- ç¨³å®šæ€§å¥½
- æ˜“äºå®ç°

**TRPO (Trust Region Policy Optimization)**ï¼š
- ä¿¡èµ–åŒºåŸŸç­–ç•¥ä¼˜åŒ–
- ç†è®ºä¿è¯
- è®¡ç®—å¤æ‚

**SAC (Soft Actor-Critic)**ï¼š
- æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ 
- è¿ç»­æ§åˆ¶æ•ˆæœå¥½
- é‡‡æ ·æ•ˆç‡é«˜

## ğŸ® åº”ç”¨é¢†åŸŸ

### 1. æ¸¸æˆAI
- **AlphaGo**ï¼šå‡»è´¥ä¸–ç•Œå›´æ£‹å† å†›
- **OpenAI Five**ï¼šDota 2 AI
- **AlphaStar**ï¼šæ˜Ÿé™…äº‰éœ¸II

### 2. æœºå™¨äººæ§åˆ¶
- **æœºæ¢°è‡‚æ“ä½œ**ï¼šæŠ“å–ã€æ”¾ç½®
- **ç§»åŠ¨æœºå™¨äºº**ï¼šå¯¼èˆªã€é¿éšœ
- **æ— äººæœº**ï¼šé£è¡Œæ§åˆ¶

### 3. è‡ªåŠ¨é©¾é©¶
- **å†³ç­–è§„åˆ’**ï¼šè·¯å¾„é€‰æ‹©ã€é€Ÿåº¦æ§åˆ¶
- **äº¤é€šä¿¡å·ä¼˜åŒ–**
- **è½¦é˜Ÿè°ƒåº¦**

### 4. æ¨èç³»ç»Ÿ
- **ä¸ªæ€§åŒ–æ¨è**ï¼šæœ€å¤§åŒ–ç”¨æˆ·æ»¡æ„åº¦
- **å¹¿å‘ŠæŠ•æ”¾**ï¼šä¼˜åŒ–ç‚¹å‡»ç‡
- **å†…å®¹æ¨é€**ï¼šç”¨æˆ·å‚ä¸åº¦

### 5. èµ„æºç®¡ç†
- **æ•°æ®ä¸­å¿ƒ**ï¼šèƒ½è€—ä¼˜åŒ–
- **ç½‘ç»œè·¯ç”±**ï¼šæµé‡è°ƒåº¦
- **é‡‘èäº¤æ˜“**ï¼šæŠ•èµ„ç­–ç•¥

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

### 1. æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡

**Îµ-è´ªå¿ƒç­–ç•¥**ï¼š
```python
def epsilon_greedy(action_space, q_values, epsilon):
    if np.random.random() < epsilon:
        return np.random.choice(action_space)  # æ¢ç´¢
    else:
        return np.argmax(q_values)  # åˆ©ç”¨
```

**è½¯æ¢ç´¢ (Softmax)**ï¼š
```python
def softmax_exploration(q_values, temperature):
    probs = np.exp(q_values / temperature) / np.sum(np.exp(q_values / temperature))
    return np.random.choice(len(probs), p=probs)
```

### 2. å¥–åŠ±è®¾è®¡
- **ç¨€ç–å¥–åŠ±**ï¼šåªåœ¨ä»»åŠ¡å®Œæˆæ—¶ç»™äºˆå¥–åŠ±
- **å¯†é›†å¥–åŠ±**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½ç»™äºˆåé¦ˆ
- **å¥–åŠ±å¡‘å½¢**ï¼šå¼•å¯¼å­¦ä¹ æ–¹å‘

### 3. ç»éªŒå›æ”¾
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
```

### 4. ç½‘ç»œæ¶æ„è®¾è®¡
- **å·ç§¯å±‚**ï¼šå¤„ç†å›¾åƒè¾“å…¥
- **å¾ªç¯å±‚**ï¼šå¤„ç†åºåˆ—æ•°æ®
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šå¤„ç†å¤æ‚çŠ¶æ€

## ğŸ§ª å®æˆ˜ç¤ºä¾‹ï¼šCartPole

```python
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.net(x)

def train_cartpole():
    env = gym.make('CartPole-v1')
    policy = PolicyNetwork(4, 2)
    optimizer = optim.Adam(policy.parameters(), lr=0.01)

    for episode in range(1000):
        state = env.reset()[0]
        rewards = []
        log_probs = []

        while True:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs = policy(state_tensor)
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample()

            next_state, reward, done, truncated, _ = env.step(action.item())

            rewards.append(reward)
            log_probs.append(action_dist.log_prob(action))

            state = next_state

            if done or truncated:
                break

        # è®¡ç®—æŠ˜æ‰£å¥–åŠ±
        discounted_rewards = []
        R = 0
        for r in reversed(rewards):
            R = r + 0.99 * R
            discounted_rewards.insert(0, R)

        discounted_rewards = torch.FloatTensor(discounted_rewards)
        log_probs = torch.stack(log_probs)

        # ç­–ç•¥æ¢¯åº¦
        loss = -torch.mean(log_probs * discounted_rewards)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if episode % 100 == 0:
            print(f"Episode {episode}, Total Reward: {sum(rewards)}")
```

## âš ï¸ æŒ‘æˆ˜å’Œé™åˆ¶

### 1. æ ·æœ¬æ•ˆç‡
- **é—®é¢˜**ï¼šéœ€è¦å¤§é‡äº¤äº’æ•°æ®
- **è§£å†³æ–¹æ¡ˆ**ï¼šæ¨¡å‹é¢„æµ‹ã€è¿ç§»å­¦ä¹ 

### 2. ç¨³å®šæ€§
- **é—®é¢˜**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®š
- **è§£å†³æ–¹æ¡ˆ**ï¼šç›®æ ‡ç½‘ç»œã€æ¢¯åº¦è£å‰ª

### 3. æ³›åŒ–èƒ½åŠ›
- **é—®é¢˜**ï¼šéš¾ä»¥é€‚åº”æ–°ç¯å¢ƒ
- **è§£å†³æ–¹æ¡ˆ**ï¼šé¢†åŸŸè‡ªé€‚åº”ã€å…ƒå­¦ä¹ 

### 4. å®‰å…¨æ€§
- **é—®é¢˜**ï¼šæ¢ç´¢å¯èƒ½å¯¼è‡´å±é™©è¡Œä¸º
- **è§£å†³æ–¹æ¡ˆ**ï¼šçº¦æŸå­¦ä¹ ã€å®‰å…¨ç­–ç•¥

## ğŸš€ å‘å±•è¶‹åŠ¿

### 1. ç¦»çº¿å¼ºåŒ–å­¦ä¹ 
- ä»å›ºå®šæ•°æ®é›†å­¦ä¹ 
- é¿å…åœ¨çº¿æ¢ç´¢é£é™©

### 2. å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ 
- åä½œå’Œç«äº‰
- å¤æ‚ç¤¾ä¼šè¡Œä¸º

### 3. å› æœå¼ºåŒ–å­¦ä¹ 
- è€ƒè™‘å› æœå…³ç³»
- æé«˜å¯è§£é‡Šæ€§

### 4. äººç±»åå¥½å­¦ä¹ 
- ä»äººç±»åé¦ˆå­¦ä¹ 
- å¯¹é½äººç±»ä»·å€¼è§‚

---

*å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½æœ€æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸä¹‹ä¸€ï¼Œå®ƒæ­£åœ¨æ”¹å˜æˆ‘ä»¬ä¸æœºå™¨äº¤äº’çš„æ–¹å¼ï¼*