#!/usr/bin/env python3
"""
äº§å“åˆ†ç±»æ¨ç†è„šæœ¬
ç”¨äºæµ‹è¯•å’Œè¿è¡Œäº§å“åˆ†ç±»æ¨¡å‹æ¨ç†
"""
import os
import sys
import argparse
import json
import logging
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from inference import ProductInference, get_inference_instance

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def single_prediction(inference, product_name: str, return_prob: bool = False):
    """å•ä¸ªäº§å“é¢„æµ‹"""
    print(f"\nğŸ” äº§å“åˆ†ç±»é¢„æµ‹")
    print("="*50)
    print(f"äº§å“åç§°: {product_name}")
    print("-"*50)

    result = inference.predict(product_name, return_prob=return_prob)

    if 'error' in result:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {result['error']}")
        return

    print(f"âœ… é¢„æµ‹ç»“æœ:")
    print(f"  ğŸ“¦ æ ‡å‡†åç§°: {result['standard_name']}")
    print(f"  ğŸ“‚ ä¸€çº§åˆ†ç±»: {result['level1_category']}")
    print(f"  ğŸ“ äºŒçº§åˆ†ç±»: {result['level2_category']}")
    print(f"  ğŸ“„ ä¸‰çº§åˆ†ç±»: {result['level3_category']}")

    if return_prob:
        print(f"\nğŸ¯ ç½®ä¿¡åº¦:")
        print(f"  æ ‡å‡†åç§°: {result.get('confidence_standard', 0):.3f}")
        print(f"  ä¸€çº§åˆ†ç±»: {result.get('confidence_level1', 0):.3f}")
        print(f"  äºŒçº§åˆ†ç±»: {result.get('confidence_level2', 0):.3f}")
        print(f"  ä¸‰çº§åˆ†ç±»: {result.get('confidence_level3', 0):.3f}")
        print(f"  ç»¼åˆç½®ä¿¡åº¦: {result.get('overall_confidence', 0):.3f}")

    print(f"â±ï¸  å“åº”æ—¶é—´: {result.get('response_time', 'N/A')}")
    print(f"ğŸ”¤ å¤„ç†åæ–‡æœ¬: {result.get('processed_text', 'N/A')}")


def top_k_prediction(inference, product_name: str, k: int = 5):
    """Top-Ké¢„æµ‹"""
    print(f"\nğŸ¯ Top-{k} é¢„æµ‹ç»“æœ")
    print("="*50)
    print(f"äº§å“åç§°: {product_name}")
    print("-"*50)

    result = inference.get_top_k_predictions(product_name, k=k)

    if 'error' in result:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {result['error']}")
        return

    for task in ['standard', 'level1', 'level2', 'level3']:
        if task in result:
            print(f"\nğŸ“Š {task.upper()} åˆ†ç±» Top-{k}:")
            for i, item in enumerate(result[task], 1):
                prob = item['probability']
                bar_length = int(prob * 20)
                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                print(f"  {i:2d}. {item['label']:<30} |{bar}| {prob:.4f}")


def batch_prediction(inference, input_file: str, output_file: str = None):
    """æ‰¹é‡é¢„æµ‹"""
    print(f"\nğŸ“¦ æ‰¹é‡é¢„æµ‹")
    print("="*50)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")

    try:
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            if input_file.endswith('.json'):
                # JSONæ ¼å¼
                data = json.load(f)
                if isinstance(data, list):
                    product_names = data
                elif isinstance(data, dict) and 'products' in data:
                    product_names = data['products']
                else:
                    raise ValueError("ä¸æ”¯æŒçš„JSONæ ¼å¼")
            else:
                # æ–‡æœ¬æ ¼å¼ (æ¯è¡Œä¸€ä¸ªäº§å“åç§°)
                product_names = [line.strip() for line in f if line.strip()]

        print(f"äº§å“æ•°é‡: {len(product_names)}")

        # æ‰¹é‡é¢„æµ‹
        print("å¼€å§‹é¢„æµ‹...")
        results = []
        for i, product_name in enumerate(product_names, 1):
            print(f"è¿›åº¦: {i}/{len(product_names)} - {product_name[:30]}...")
            result = inference.predict(product_name, return_prob=True)
            results.append(result)

        # ä¿å­˜ç»“æœ
        if output_file:
            output_data = {
                'input_file': input_file,
                'total_products': len(product_names),
                'predictions': results
            }

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        if 'overall_confidence' in results[0]:
            confidences = [r['overall_confidence'] for r in results]
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {sum(confidences)/len(confidences):.3f}")
            print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max(confidences):.3f}")
            print(f"  æœ€ä½ç½®ä¿¡åº¦: {min(confidences):.3f}")

    except Exception as e:
        print(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")


def performance_test(inference, num_samples: int = 100):
    """æ€§èƒ½æµ‹è¯•"""
    print(f"\nâš¡ æ€§èƒ½æµ‹è¯•")
    print("="*50)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples}")

    performance = inference.evaluate_performance(num_samples)

    print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"  ğŸ“ˆ å¹³å‡å“åº”æ—¶é—´: {performance['avg_time_ms']:.2f} ms")
    print(f"  âš¡ æœ€å¿«å“åº”æ—¶é—´: {performance['min_time_ms']:.2f} ms")
    print(f"  ğŸŒ æœ€æ…¢å“åº”æ—¶é—´: {performance['max_time_ms']:.2f} ms")
    print(f"  ğŸ“Š æ ‡å‡†å·®: {performance['std_time_ms']:.2f} ms")
    print(f"  ğŸ“ˆ ä¸­ä½æ•°: {performance['median_time_ms']:.2f} ms")
    print(f"  ğŸ“‰ P95: {performance['p95_time_ms']:.2f} ms")
    print(f"  ğŸš€ ååé‡: {performance['throughput_qps']:.2f} QPS")

    # æ€§èƒ½è¯„ä¼°
    avg_time = performance['avg_time_ms']
    if avg_time < 100:
        print(f"  âœ… æ€§èƒ½ä¼˜ç§€ ({avg_time:.1f}ms < 100ms)")
    elif avg_time < 500:
        print(f"  ğŸŸ¡ æ€§èƒ½è‰¯å¥½ ({avg_time:.1f}ms < 500ms)")
    elif avg_time < 1000:
        print(f"  ğŸŸ  æ€§èƒ½ä¸€èˆ¬ ({avg_time:.1f}ms < 1000ms)")
    else:
        print(f"  âŒ æ€§èƒ½è¾ƒå·® ({avg_time:.1f}ms > 1000ms)")


def interactive_mode(inference):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\nğŸ® äº¤äº’å¼æ¨¡å¼")
    print("="*50)
    print("è¾“å…¥äº§å“åç§°è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("è¾“å…¥ 'top-k <k>' è¿›è¡ŒTop-Ké¢„æµ‹")
    print("è¾“å…¥ 'perf' è¿›è¡Œæ€§èƒ½æµ‹è¯•")
    print("-"*50)

    while True:
        try:
            user_input = input("\näº§å“åç§° > ").strip()

            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§!")
                break

            if user_input.lower() == 'perf':
                performance_test(inference)
                continue

            if user_input.lower().startswith('top-k'):
                parts = user_input.split()
                k = int(parts[1]) if len(parts) > 1 else 3
                product_name = input("è¯·è¾“å…¥äº§å“åç§°: ").strip()
                if product_name:
                    top_k_prediction(inference, product_name, k)
                continue

            if user_input:
                single_prediction(inference, user_input, return_prob=True)

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§!")
            break
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='äº§å“åˆ†ç±»æ¨¡å‹æ¨ç†è„šæœ¬')

    parser.add_argument('--model_path', type=str, default='./models/best_model.pt',
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--product', type=str, help='å•ä¸ªäº§å“åç§°')
    parser.add_argument('--top_k', type=int, default=5, help='Top-Kæ•°é‡')
    parser.add_argument('--batch_input', type=str, help='æ‰¹é‡é¢„æµ‹è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--batch_output', type=str, help='æ‰¹é‡é¢„æµ‹è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--perf_test', action='store_true', help='æ€§èƒ½æµ‹è¯•')
    parser.add_argument('--perf_samples', type=int, default=100, help='æ€§èƒ½æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼æ¨¡å¼')

    args = parser.parse_args()

    print("ğŸ¤– äº§å“åˆ†ç±»æ¨¡å‹æ¨ç†å·¥å…·")
    print("="*50)

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        print("è¯·ç¡®ä¿:")
        print("  1. æ¨¡å‹è®­ç»ƒå·²å®Œæˆ")
        print("  2. æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜åˆ°æ­£ç¡®ä½ç½®")
        return 1

    # åˆ›å»ºæ¨ç†å™¨
    try:
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        inference = ProductInference(args.model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return 1

    # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒæ“ä½œ
    if args.product:
        # å•ä¸ªé¢„æµ‹
        single_prediction(inference, args.product, return_prob=True)

    elif args.top_k and args.product:
        # Top-Ké¢„æµ‹
        top_k_prediction(inference, args.product, args.top_k)

    elif args.batch_input:
        # æ‰¹é‡é¢„æµ‹
        output_file = args.batch_output or f"{args.batch_input}_results.json"
        batch_prediction(inference, args.batch_input, output_file)

    elif args.perf_test:
        # æ€§èƒ½æµ‹è¯•
        performance_test(inference, args.perf_samples)

    elif args.interactive:
        # äº¤äº’å¼æ¨¡å¼
        interactive_mode(inference)

    else:
        # é»˜è®¤ï¼šç¤ºä¾‹é¢„æµ‹
        print("ğŸ“ è¿è¡Œç¤ºä¾‹é¢„æµ‹...")
        examples = [
            "è‹¹æœiPhone 14 Proæ‰‹æœº",
            "åä¸ºMateBook X Proç¬”è®°æœ¬ç”µè„‘",
            "å°ç±³65å¯¸æ™ºèƒ½ç”µè§†"
        ]

        for example in examples:
            single_prediction(inference, example, return_prob=True)

        print(f"\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print(f"  --product 'äº§å“åç§°'        : å•ä¸ªäº§å“é¢„æµ‹")
        print(f"  --top_k 3 --product 'åç§°'  : Top-Ké¢„æµ‹")
        print(f"  --batch_input file.txt     : æ‰¹é‡é¢„æµ‹")
        print(f"  --perf_test                : æ€§èƒ½æµ‹è¯•")
        print(f"  --interactive              : äº¤äº’å¼æ¨¡å¼")

    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)