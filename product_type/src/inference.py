"""
æ¨ç†æ¨¡å— - äº§å“åˆ†ç±»æ¨¡å‹æ¨ç†æ¥å£
"""
import torch
import torch.nn as nn
from transformers import BertTokenizer
from modelscope_utils import load_tokenizer
import json
import logging
from typing import Dict, Tuple, Optional, List
import os
from pathlib import Path
import numpy as np
import time

from model import MultiTaskProductClassifier

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProductInference:
    """äº§å“åˆ†ç±»æ¨ç†å™¨"""

    def __init__(self, model_path: str = "./models/best_model"):
        """
        åˆå§‹åŒ–æ¨ç†å™¨

        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.model_path = model_path
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model = None
        self.tokenizer = None
        self.label_mappings = None

        self._load_model()
        self._load_tokenizer()
        self._load_label_mappings()

        logger.info(f"æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•æ ¼å¼ï¼ˆæ–°ä¿å­˜æ ¼å¼ï¼‰
            if os.path.isdir(self.model_path):
                logger.info(f"æ£€æµ‹åˆ°ç›®å½•æ ¼å¼æ¨¡å‹: {self.model_path}")

                # ä½¿ç”¨æ–°çš„from_saved_modelæ–¹æ³•åŠ è½½
                self.model = MultiTaskProductClassifier.from_saved_model(
                    self.model_path)

            else:
                # å…¼å®¹æ—§çš„å•æ–‡ä»¶æ ¼å¼
                logger.info(f"æ£€æµ‹åˆ°å•æ–‡ä»¶æ ¼å¼æ¨¡å‹: {self.model_path}")

                # åŠ è½½æ£€æŸ¥ç‚¹
                checkpoint = torch.load(
                    self.model_path, map_location=self.device)

                # è·å–æ¨¡å‹é…ç½®
                if 'config' in checkpoint:
                    # å¦‚æœé…ç½®å·²ä¿å­˜
                    from model import ProductClassifierConfig
                    config = ProductClassifierConfig()
                    config.__dict__.update(checkpoint['config'])
                else:
                    # ä½¿ç”¨é»˜è®¤é…ç½®
                    from model import ProductClassifierConfig
                    config = ProductClassifierConfig()

                # åˆ›å»ºæ¨¡å‹ï¼ˆç›´æ¥å®ä¾‹åŒ–ï¼Œé¿å…from_pretrainedï¼‰
                from transformers import BertConfig

                # åˆ›å»ºé…ç½®
                bert_config = BertConfig(
                    hidden_size=768,
                    num_hidden_layers=12,
                    num_attention_heads=12,
                    intermediate_size=3072,
                    hidden_dropout_prob=config.hidden_dropout_prob,
                    attention_probs_dropout_prob=config.hidden_dropout_prob,
                    max_position_embeddings=config.max_length +
                    2,  # +2 for [CLS] and [SEP]
                    vocab_size=21128,  # ä¸­æ–‡BERTè¯æ±‡è¡¨å¤§å°
                )

                # æ·»åŠ è‡ªå®šä¹‰é…ç½®
                bert_config.num_labels_standard = config.num_labels_standard
                bert_config.num_labels_level1 = config.num_labels_level1
                bert_config.num_labels_level2 = config.num_labels_level2
                bert_config.num_labels_level3 = config.num_labels_level3
                bert_config.loss_weights = config.loss_weights

                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                self.model = MultiTaskProductClassifier(bert_config)

                # åŠ è½½æƒé‡
                if torch.cuda.device_count() > 1:
                    # å¤„ç†å¤šGPUä¿å­˜çš„æ¨¡å‹
                    state_dict = {}
                    for k, v in checkpoint['model_state_dict'].items():
                        name = k.replace('module.', '') if k.startswith(
                            'module.') else k
                        state_dict[name] = v
                else:
                    state_dict = checkpoint['model_state_dict']

                self.model.load_state_dict(state_dict)

            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.model.to(self.device)
            self.model.eval()

            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ¥è‡ª: {self.model_path}")

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _load_tokenizer(self):
        """åŠ è½½åˆ†è¯å™¨"""
        try:
            tokenizer_path = "./models/tokenizer"
            if os.path.exists(tokenizer_path):
                self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)
            else:
                # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
                self.tokenizer = load_tokenizer(
                    "dienstag/chinese-bert-wwm-ext")
                logger.warning("æœªæ‰¾åˆ°æœ¬åœ°åˆ†è¯å™¨ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")

        except Exception as e:
            logger.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            raise

    def _load_label_mappings(self):
        """åŠ è½½æ ‡ç­¾æ˜ å°„"""
        try:
            mapping_path = "./models/label_mappings.json"
            if not os.path.exists(mapping_path):
                raise FileNotFoundError(f"æ ‡ç­¾æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_path}")

            with open(mapping_path, 'r', encoding='utf-8') as f:
                self.label_mappings = json.load(f)

            # åˆ›å»ºåå‘æ˜ å°„ä»¥æé«˜æŸ¥æ‰¾æ•ˆç‡
            self.reverse_mappings = {}
            task_mappings = {
                'standard': 'standard_name',
                'level1': 'level1_category',
                'level2': 'level2_category',
                'level3': 'level3_category'
            }

            for label_type, mapping_key in task_mappings.items():
                if mapping_key in self.label_mappings:
                    reverse_key = f"{label_type}_reverse_mapping"
                    self.reverse_mappings[reverse_key] = {str(idx): name for name, idx in self.label_mappings[mapping_key].items()}
                    logger.info(f"åˆ›å»ºåå‘æ˜ å°„: {reverse_key}, åŒ…å« {len(self.reverse_mappings[reverse_key])} ä¸ªæ ‡ç­¾")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ˜ å°„é”®: {mapping_key}")

            logger.info("æ ‡ç­¾æ˜ å°„åŠ è½½æˆåŠŸ")

        except Exception as e:
            logger.error(f"æ ‡ç­¾æ˜ å°„åŠ è½½å¤±è´¥: {e}")
            raise

    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        try:
            import jieba
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.lcut(str(text))
            # è¿‡æ»¤æ‰å•å­—ç¬¦ï¼ˆé™¤éæ˜¯æ•°å­—æˆ–å­—æ¯ï¼‰
            words = [word for word in words if len(
                word) > 1 or word.isdigit() or word.isalpha()]
            return ' '.join(words)
        except ImportError:
            logger.warning("jiebaæœªå®‰è£…ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
            return str(text)

    def predict(self, product_name: str, return_prob: bool = False) -> Dict:
        """
        é¢„æµ‹äº§å“åˆ†ç±»

        Args:
            product_name: äº§å“åç§°
            return_prob: æ˜¯å¦è¿”å›æ¦‚ç‡

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()

        try:
            # é¢„å¤„ç†æ–‡æœ¬
            preprocessed_name = self._preprocess_text(product_name)

            # åˆ†è¯å’Œç¼–ç 
            encoding = self.tokenizer(
                preprocessed_name,
                truncation=True,
                padding='max_length',
                max_length=128,
                return_tensors='pt',
            )

            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)

            # é¢„æµ‹
            with torch.no_grad():
                if return_prob:
                    # å¸¦ç½®ä¿¡åº¦çš„é¢„æµ‹
                    result = self.model.predict_with_prob(
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )
                else:
                    # ç®€å•é¢„æµ‹
                    result = self.model.predict(
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )

            # è§£ç ç»“æœ
            if return_prob:
                predictions = {
                    'standard_name': self._decode_label(result['standard'][0], 'standard'),
                    'level1_category': self._decode_label(result['level1'][0], 'level1'),
                    'level2_category': self._decode_label(result['level2'][0], 'level2'),
                    'level3_category': self._decode_label(result['level3'][0], 'level3'),
                    'confidence_standard': float(result['standard'][1]),
                    'confidence_level1': float(result['level1'][1]),
                    'confidence_level2': float(result['level2'][1]),
                    'confidence_level3': float(result['level3'][1]),
                }

                # ç»¼åˆç½®ä¿¡åº¦
                predictions['overall_confidence'] = (
                    predictions['confidence_standard'] * 0.4 +
                    predictions['confidence_level1'] * 0.2 +
                    predictions['confidence_level2'] * 0.2 +
                    predictions['confidence_level3'] * 0.2
                )
            else:
                predictions = {
                    'standard_name': self._decode_label(result[0], 'standard'),
                    'level1_category': self._decode_label(result[1], 'level1'),
                    'level2_category': self._decode_label(result[2], 'level2'),
                    'level3_category': self._decode_label(result[3], 'level3'),
                }

            # æ·»åŠ å…ƒä¿¡æ¯
            predictions['input_text'] = product_name
            predictions['processed_text'] = preprocessed_name
            predictions['response_time'] = f"{(time.time() - start_time)*1000:.2f}ms"

            return predictions

        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return {
                'error': str(e),
                'input_text': product_name,
                'response_time': f"{(time.time() - start_time)*1000:.2f}ms"
            }

    def _decode_label(self, label_idx: int, label_type: str) -> str:
        """è§£ç æ ‡ç­¾"""
        try:
            # ç¡®ä¿label_idxæ˜¯æ•´æ•°
            label_idx_int = int(label_idx)

            # ä½¿ç”¨é¢„åˆ›å»ºçš„åå‘æ˜ å°„è¿›è¡Œå¿«é€ŸæŸ¥æ‰¾
            reverse_key = f"{label_type}_reverse_mapping"
            if reverse_key in self.reverse_mappings:
                result = self.reverse_mappings[reverse_key].get(str(label_idx_int))
                if result:
                    return result
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ ‡ç­¾ç´¢å¼•: {label_idx_int}, ç±»å‹: {label_type}")
                    return f"æœªçŸ¥æ ‡ç­¾_{label_idx_int}"
            else:
                logger.warning(f"æœªæ‰¾åˆ°åå‘æ˜ å°„ç±»å‹: {label_type}")
                return f"æœªçŸ¥æ ‡ç­¾ç±»å‹: {label_type}"

        except Exception as e:
            logger.error(f"æ ‡ç­¾è§£ç é”™è¯¯: {e}, label_idx={label_idx}, label_type={label_type}")
            return f"è§£ç é”™è¯¯_{label_idx}"

    def predict_batch(self, product_names: List[str], return_prob: bool = False) -> List[Dict]:
        """
        æ‰¹é‡é¢„æµ‹

        Args:
            product_names: äº§å“åç§°åˆ—è¡¨
            return_prob: æ˜¯å¦è¿”å›æ¦‚ç‡

        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for product_name in product_names:
            result = self.predict(product_name, return_prob)
            results.append(result)

        return results

    def get_top_k_predictions(self, product_name: str, k: int = 5) -> Dict:
        """
        è·å–Top-Ké¢„æµ‹ç»“æœ

        Args:
            product_name: äº§å“åç§°
            k: è¿”å›çš„topæ•°é‡

        Returns:
            åŒ…å«top-kç»“æœçš„å­—å…¸
        """
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            preprocessed_name = self._preprocess_text(product_name)

            # åˆ†è¯å’Œç¼–ç 
            encoding = self.tokenizer(
                preprocessed_name,
                truncation=True,
                padding='max_length',
                max_length=128,
                return_tensors='pt',
            )

            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)

            # è·å–é¢„æµ‹æ¦‚ç‡
            with torch.no_grad():
                result = self.model.predict_with_prob(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

            # è·å–Top-Kç»“æœ
            top_k_results = {}
            for task in ['standard', 'level1', 'level2', 'level3']:
                if task in result['probs']:
                    probs = result['probs'][task].cpu().numpy()[0]
                    top_k_indices = np.argsort(probs)[-k:][::-1]

                    top_k_results[task] = []
                    for idx in top_k_indices:
                        label = self._decode_label(idx, task)
                        prob = float(probs[idx])
                        top_k_results[task].append({
                            'label': label,
                            'probability': prob
                        })

            # æ·»åŠ åŸºç¡€ä¿¡æ¯
            top_k_results['input_text'] = product_name
            top_k_results['processed_text'] = preprocessed_name

            return top_k_results

        except Exception as e:
            logger.error(f"Top-Ké¢„æµ‹å¤±è´¥: {e}")
            return {'error': str(e)}

    def evaluate_performance(self, num_samples: int = 100) -> Dict:
        """
        è¯„ä¼°æ¨ç†æ€§èƒ½

        Args:
            num_samples: æµ‹è¯•æ ·æœ¬æ•°

        Returns:
            æ€§èƒ½ç»Ÿè®¡ç»“æœ
        """
        import random

        # ç”Ÿæˆéšæœºæµ‹è¯•æ ·æœ¬
        test_products = [
            "è‹¹æœiPhoneæ‰‹æœº", "åä¸ºç¬”è®°æœ¬ç”µè„‘", "å°ç±³ç”µè§†", "è”æƒ³ç”µè„‘", "ä¸‰æ˜Ÿå¹³æ¿",
            "æˆ´å°”æœåŠ¡å™¨", "ç´¢å°¼ç›¸æœº", "ä½³èƒ½æ‰“å°æœº", "æƒ æ™®æ‰«æä»ª", "è·¯ç”±å™¨",
            "äº¤æ¢æœº", "æŠ•å½±ä»ª", "éŸ³å“è®¾å¤‡", "è€³æœº", "é”®ç›˜", "é¼ æ ‡"
        ]

        # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨
        while len(test_products) < num_samples:
            test_products.extend(test_products)

        test_products = test_products[:num_samples]

        # æµ‹è¯•æ¨ç†é€Ÿåº¦
        times = []
        for product_name in test_products:
            start_time = time.time()
            _ = self.predict(product_name)
            times.append((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        times = np.array(times)
        performance_stats = {
            'num_samples': num_samples,
            'avg_time_ms': float(np.mean(times)),
            'min_time_ms': float(np.min(times)),
            'max_time_ms': float(np.max(times)),
            'std_time_ms': float(np.std(times)),
            'median_time_ms': float(np.median(times)),
            'p95_time_ms': float(np.percentile(times, 95)),
            'throughput_qps': num_samples / (np.sum(times) / 1000),  # æ¯ç§’æŸ¥è¯¢æ•°
        }

        logger.info("æ€§èƒ½è¯„ä¼°å®Œæˆ:")
        for key, value in performance_stats.items():
            logger.info(f"  {key}: {value}")

        return performance_stats


# å…¨å±€æ¨ç†å™¨å®ä¾‹
_inference_instance = None


def get_inference_instance(model_path: str = "./models/best_model") -> ProductInference:
    """è·å–æ¨ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _inference_instance
    if _inference_instance is None:
        _inference_instance = ProductInference(model_path)
    return _inference_instance


if __name__ == "__main__":
    # æµ‹è¯•æ¨ç†å™¨
    print("ğŸ§ª æµ‹è¯•äº§å“åˆ†ç±»æ¨ç†å™¨...")

    try:
        # åˆ›å»ºæ¨ç†å™¨
        inference = ProductInference()

        # æµ‹è¯•å•ä¸ªé¢„æµ‹
        test_products = [
            "æ‰‹æœ¯æ— å½±ç¯",
            "å½©è‰²è¶…å£°",
            "4Kè…¹è…”é•œ",
            "CT",
            "èƒƒé•œ"
        ]

        print("\nğŸ” å•ä¸ªé¢„æµ‹æµ‹è¯•:")
        for product in test_products:
            print(f"\näº§å“: {product}")
            result = inference.predict(product, return_prob=True)
            print(
                f"  æ ‡å‡†åç§°: {result['standard_name']} (ç½®ä¿¡åº¦: {result.get('confidence_standard', 'N/A'):.3f})")
            print(
                f"  ä¸€çº§åˆ†ç±»: {result['level1_category']} (ç½®ä¿¡åº¦: {result.get('confidence_level1', 'N/A'):.3f})")
            print(
                f"  äºŒçº§åˆ†ç±»: {result['level2_category']} (ç½®ä¿¡åº¦: {result.get('confidence_level2', 'N/A'):.3f})")
            print(
                f"  ä¸‰çº§åˆ†ç±»: {result['level3_category']} (ç½®ä¿¡åº¦: {result.get('confidence_level3', 'N/A'):.3f})")
            print(f"  ç»¼åˆç½®ä¿¡åº¦: {result.get('overall_confidence', 'N/A'):.3f}")
            print(f"  å“åº”æ—¶é—´: {result['response_time']}")

        # æµ‹è¯•æ‰¹é‡é¢„æµ‹
        print("\nğŸ“Š æ‰¹é‡é¢„æµ‹æµ‹è¯•:")
        batch_results = inference.predict_batch(
            test_products[:3], return_prob=True)
        for i, result in enumerate(batch_results):
            print(
                f"  äº§å“{i+1}: {result['standard_name']} - {result.get('overall_confidence', 'N/A'):.3f}")

        # æµ‹è¯•Top-Ké¢„æµ‹
        print("\nğŸ¯ Top-Ké¢„æµ‹æµ‹è¯•:")
        top_k_result = inference.get_top_k_predictions(test_products[0], k=3)
        print(f"äº§å“: {top_k_result['input_text']}")
        print("Top-3 æ ‡å‡†åç§°:")
        for i, item in enumerate(top_k_result['standard']):
            print(f"  {i+1}. {item['label']} - {item['probability']:.4f}")

        # æ€§èƒ½æµ‹è¯•
        print("\nâš¡ æ€§èƒ½æµ‹è¯•:")
        performance = inference.evaluate_performance(num_samples=50)
        print(f"  å¹³å‡å“åº”æ—¶é—´: {performance['avg_time_ms']:.2f}ms")
        print(f"  ååé‡: {performance['throughput_qps']:.2f} QPS")

        print("\nâœ… æ¨ç†å™¨æµ‹è¯•é€šè¿‡!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿:")
        print("  1. æ¨¡å‹æ–‡ä»¶å·²ç”Ÿæˆ (./models/best_model/)")
        print("  2. æ ‡ç­¾æ˜ å°„æ–‡ä»¶å·²ç”Ÿæˆ (./models/label_mappings.json)")
        print("  3. åˆ†è¯å™¨æ–‡ä»¶å·²ä¿å­˜ (./models/tokenizer/)")
