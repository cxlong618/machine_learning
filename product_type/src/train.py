#!/usr/bin/env python3
"""
æœ€ç»ˆè®­ç»ƒè„šæœ¬ - äº§å“åˆ†ç±»å¤šä»»åŠ¡BERTæ¨¡å‹
å®Œå…¨ä½¿ç”¨ModelScopeï¼Œæ— HuggingFaceä¾èµ–
"""
import os
import sys
import logging
os.environ['TORCH_CUDA_ARCH_LIST'] = '8.9'

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def check_data_files(train_path=None, val_path=None):
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    logger.info("æ£€æŸ¥æ•°æ®æ–‡ä»¶...")

    if train_path and val_path:
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        required_files = [train_path, val_path]
    else:
        # ä½¿ç”¨é»˜è®¤è·¯å¾„
        required_files = ['data/train.csv', 'data/val.csv']

    missing_files = [f for f in required_files if not os.path.exists(f)]

    if missing_files:
        logger.error(f"ç¼ºå¤±æ•°æ®æ–‡ä»¶: {missing_files}")
        logger.info("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨:")
        for f in missing_files:
            logger.info(f"   - {f}")
        return False

    logger.info("æ•°æ®æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='äº§å“åˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--train_path', type=str, required=True, help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--val_path', type=str, required=True, help='éªŒè¯æ•°æ®CSVæ–‡ä»¶è·¯å¾„')

    # å¯é€‰å‚æ•°
    parser.add_argument('--max_length', type=int, default=128, help='æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 128)')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)')
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='å­¦ä¹ ç‡ (é»˜è®¤: 2e-5)')
    parser.add_argument('--num_epochs', type=int, default=10, help='è®­ç»ƒè½®æ•° (é»˜è®¤: 10)')
    parser.add_argument('--model_name', type=str, default='dienstag/chinese-bert-wwm-ext', help='åŸºç¡€æ¨¡å‹åç§°')
    parser.add_argument('--warmup_steps', type=int, default=500, help='é¢„çƒ­æ­¥æ•° (é»˜è®¤: 500)')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡ (é»˜è®¤: 0.01)')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (é»˜è®¤: 1)')

    return parser.parse_args()


def run_training():
    """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
    args = parse_arguments()

    logger.info("å¯åŠ¨äº§å“åˆ†ç±»æ¨¡å‹è®­ç»ƒ...")
    logger.info("=" * 60)

    # æ˜¾ç¤ºè®­ç»ƒå‚æ•°
    logger.info("è®­ç»ƒå‚æ•°:")
    logger.info(f"  è®­ç»ƒæ•°æ®: {args.train_path}")
    logger.info(f"  éªŒè¯æ•°æ®: {args.val_path}")
    logger.info(f"  æœ€å¤§é•¿åº¦: {args.max_length}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    logger.info(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    logger.info(f"  åŸºç¡€æ¨¡å‹: {args.model_name}")
    logger.info(f"  æƒé‡è¡°å‡: {args.weight_decay}")
    logger.info("=" * 60)

    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader
        from torch.optim import AdamW
        from transformers import get_linear_schedule_with_warmup
        from sklearn.metrics import classification_report, accuracy_score
        import numpy as np
        from datetime import datetime
        import json
        from tqdm import tqdm
        import wandb

        # å¯¼å…¥é¡¹ç›®æ¨¡å—
        from dataset import ProductDataset, DataCollator
        from model import MultiTaskProductClassifier
        from modelscope_utils import load_tokenizer, load_bert_model

        logger.info("æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")

        # å¼ºåˆ¶ä½¿ç”¨GPUå¹¶ä¼˜åŒ–é…ç½®
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_props = torch.cuda.get_device_properties(0)
            total_memory_gb = gpu_props.total_memory / 1e9
            gpu_name = torch.cuda.get_device_name(0)

            logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
            logger.info(f"GPU: {gpu_name}")
            logger.info(f"æ˜¾å­˜: {total_memory_gb:.1f} GB")

            # æ ¹æ®æ˜¾å­˜å¤§å°è‡ªåŠ¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
            if total_memory_gb >= 24:
                # 24GB+ æ˜¾å­˜ï¼Œä½¿ç”¨å¤§æ‰¹æ¬¡
                optimal_batch_size = min(args.batch_size, 64)
                logger.info(f"å¤§æ˜¾å­˜GPUï¼Œè®¾ç½®æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
            elif total_memory_gb >= 16:
                # 16-24GB æ˜¾å­˜
                optimal_batch_size = min(args.batch_size, 32)
                logger.info(f"ä¸­ç­‰æ˜¾å­˜GPUï¼Œè®¾ç½®æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
            elif total_memory_gb >= 8:
                # 8-16GB æ˜¾å­˜
                optimal_batch_size = min(args.batch_size, 16)
                logger.info(f"å°æ˜¾å­˜GPUï¼Œè®¾ç½®æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
            else:
                # å°äº8GBæ˜¾å­˜
                optimal_batch_size = min(args.batch_size, 8)
                logger.warning(f"å¾ˆå°æ˜¾å­˜GPU({total_memory_gb:.1f}GB)ï¼Œè®¾ç½®å°æ‰¹æ¬¡: {optimal_batch_size}")

            args.batch_size = optimal_batch_size

            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            logger.info("GPUç¼“å­˜å·²æ¸…ç†")

        else:
            logger.error("æœªæ£€æµ‹åˆ°CUDAï¼Œè¯·ç¡®ä¿å®‰è£…äº†GPUç‰ˆæœ¬çš„PyTorch")
            device = torch.device('cpu')
            # GPUä¸å¯ç”¨æ—¶ä¹Ÿä½¿ç”¨ç›¸å¯¹åˆç†çš„æ‰¹æ¬¡
            args.batch_size = min(16, args.batch_size)

        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'

        # åˆå§‹åŒ–WandBï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
        os.environ['WANDB_MODE'] = 'offline'
        wandb.init(
            project="product-classification",
            name=f"training-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            config=vars(args),
            mode="offline"
        )

        # åŠ è½½åˆ†è¯å™¨
        logger.info("åŠ è½½åˆ†è¯å™¨...")
        tokenizer = load_tokenizer(args.model_name)

        # åˆ›å»ºæ•°æ®é›†
        logger.info("åˆ›å»ºæ•°æ®é›†...")
        train_dataset = ProductDataset(
            data_path=args.train_path,
            tokenizer=tokenizer,
            max_length=args.max_length,
            is_train=True
        )

        val_dataset = ProductDataset(
            data_path=args.val_path,
            tokenizer=tokenizer,
            max_length=args.max_length,
            is_train=False
        )

        logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")

        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        data_collator = DataCollator(tokenizer, max_length=args.max_length)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - GPUä¼˜åŒ–
        pin_memory = torch.cuda.is_available()
        num_workers = 0  # Windowsä¸‹ä¿æŒ0é¿å…é—®é¢˜

        train_dataloader = DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=True,  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡ï¼Œæé«˜GPUåˆ©ç”¨ç‡
            collate_fn=data_collator,
            persistent_workers=False  # é¿å…å†…å­˜æ³„æ¼
        )

        val_dataloader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=False,
            collate_fn=data_collator,
            persistent_workers=False
        )

        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model_config = {
            'num_labels_standard': len(train_dataset.standard_mapping),
            'num_labels_level1': len(train_dataset.level1_mapping),
            'num_labels_level2': len(train_dataset.level2_mapping),
            'num_labels_level3': len(train_dataset.level3_mapping),
            'loss_weights': {'standard': 0.4, 'level1': 0.2, 'level2': 0.2, 'level3': 0.2}
        }

        model = MultiTaskProductClassifier.from_pretrained(
            args.model_name,
            **model_config
        )
        model.to(device)

        logger.info(f"æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}")

        # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = AdamW(
            model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )

        total_steps = len(train_dataloader) * args.num_epochs // args.gradient_accumulation_steps
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=total_steps
        )

        logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")

        # è®­ç»ƒå¾ªç¯ - GPUä¼˜åŒ–
        logger.info("å¼€å§‹è®­ç»ƒ...")
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 5  # å¢åŠ è€å¿ƒï¼Œå……åˆ†åˆ©ç”¨GPU

        # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        use_amp = torch.cuda.is_available() and hasattr(torch.cuda, 'amp')
        if use_amp:
            logger.info("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒåŠ é€Ÿ")
            scaler = torch.cuda.amp.GradScaler()

        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
        use_checkpoint = args.max_length > 128  # é•¿åºåˆ—ä½¿ç”¨æ£€æŸ¥ç‚¹

        for epoch in range(args.num_epochs):
            logger.info(f"Epoch {epoch + 1}/{args.num_epochs}")

            # é¢„çƒ­GPUï¼ˆå‰å‡ ä¸ªæ‰¹æ¬¡ä½¿ç”¨è¾ƒå°æ‰¹æ¬¡ï¼‰
            is_warmup = epoch == 0
            warmup_factor = 0.5 if is_warmup else 1.0

            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_train_loss = 0
            steps_per_epoch = len(train_dataloader)

            # ä½¿ç”¨æ›´æ¿€è¿›çš„è¿›åº¦æ¡æ›´æ–°
            train_progress = tqdm(
                train_dataloader,
                desc=f"è®­ç»ƒ Epoch {epoch + 1}",
                leave=True,
                dynamic_ncols=True
            )

            for step, batch in enumerate(train_progress):
                # åŠ¨æ€æ‰¹æ¬¡å¤§å°è°ƒæ•´ï¼ˆGPUé¢„çƒ­åä½¿ç”¨æ›´å¤§æ‰¹æ¬¡ï¼‰
                current_batch_size = int(args.batch_size * warmup_factor)
                if batch['input_ids'].size(0) != current_batch_size:
                    continue  # è·³è¿‡ä¸åŒ¹é…çš„æ‰¹æ¬¡

                try:
                    # é«˜æ•ˆæ•°æ®è½¬ç§»
                    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}

                    # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
                    if use_amp:
                        with torch.amp.autocast('cuda'):
                            outputs = model(**batch)
                            loss = outputs['loss'] / warmup_factor
                    else:
                        outputs = model(**batch)
                        loss = outputs['loss'] / warmup_factor

                except RuntimeError as e:
                    if "CUDA" in str(e):
                        logger.error(f"CUDAé”™è¯¯: {e}")
                        # æ¸…ç†GPUç¼“å­˜å¹¶é‡è¯•
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise

                # æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–
                if args.gradient_accumulation_steps > 1:
                    loss = loss / args.gradient_accumulation_steps

                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                if use_amp:
                    scaler.scale(loss).backward()
                    if (step + 1) % args.gradient_accumulation_steps == 0:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                        scaler.step(optimizer)
                        scaler.update()
                        scheduler.step()
                        optimizer.zero_grad()
                        # å®šæœŸæ¸…ç†GPUç¼“å­˜
                        if step % 50 == 0:
                            torch.cuda.empty_cache()
                else:
                    loss.backward()
                    if (step + 1) % args.gradient_accumulation_steps == 0:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                        optimizer.step()
                        scheduler.step()
                        optimizer.zero_grad()
                        # å®šæœŸæ¸…ç†GPUç¼“å­˜
                        if step % 50 == 0:
                            torch.cuda.empty_cache()

                total_train_loss += loss.item() * warmup_factor

                # æ›´é«˜æ•ˆçš„è¿›åº¦æ¡æ›´æ–°
                current_loss = loss.item() * warmup_factor
                current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else args.learning_rate
                train_progress.set_postfix({
                    'loss': f"{current_loss:.4f}",
                    'lr': f"{current_lr:.2e}",
                    'batch': f"{current_batch_size}"
                })

                # å‡å°‘æ—¥å¿—é¢‘ç‡ä»¥æé«˜æ€§èƒ½
                if step % 50 == 0:  # æ¯50æ­¥è®°å½•ä¸€æ¬¡è€Œä¸æ˜¯100æ­¥
                    wandb.log({
                        'train_loss': current_loss,
                        'learning_rate': current_lr,
                        'step': epoch * steps_per_epoch + step,
                        'epoch': epoch + 1,
                        'gpu_memory_used': torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0
                    })

            avg_train_loss = total_train_loss / len(train_dataloader)
            logger.info(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")

            # éªŒè¯é˜¶æ®µ - GPUä¼˜åŒ–
            model.eval()
            total_val_loss = 0
            all_predictions = {'standard': [], 'level1': [], 'level2': [], 'level3': []}
            all_labels = {'standard': [], 'level1': [], 'level2': [], 'level3': []}

            with torch.no_grad():
                # GPUéªŒè¯æ—¶ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
                val_batch_size = min(args.batch_size * 2, 64)  # éªŒè¯æ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡

                val_progress = tqdm(val_dataloader, desc=f"éªŒè¯ Epoch {epoch + 1}")

                for batch in val_progress:
                    try:
                        # é«˜æ•ˆæ•°æ®è½¬ç§»
                        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}

                        # ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆå¦‚æœè®­ç»ƒæ—¶ä¹Ÿä½¿ç”¨ï¼‰
                        if use_amp:
                            with torch.amp.autocast('cuda'):
                                outputs = model(**batch)
                                loss = outputs['loss']
                        else:
                            outputs = model(**batch)
                            loss = outputs['loss']

                        total_val_loss += loss.item()

                    except RuntimeError as e:
                        if "CUDA" in str(e):
                            logger.error(f"éªŒè¯CUDAé”™è¯¯: {e}")
                            torch.cuda.empty_cache()
                            continue
                        else:
                            raise

                    # æ”¶é›†é¢„æµ‹ç»“æœ - å…¼å®¹æ€§ä¿®å¤
                    for task in ['standard', 'level1', 'level2', 'level3']:
                        preds = torch.argmax(outputs[f'logits_{task}'], dim=-1)
                        labels = batch[f'labels_{task}']

                        # å…¼å®¹æ€§numpyè½¬æ¢
                        try:
                            all_predictions[task].extend(preds.cpu().numpy())
                            all_labels[task].extend(labels.cpu().numpy())
                        except RuntimeError as e:
                            if "Numpy is not available" in str(e):
                                # ä½¿ç”¨PyTorchåŸç”Ÿæ–¹æ³•
                                all_predictions[task].extend(preds.cpu().tolist())
                                all_labels[task].extend(labels.cpu().tolist())
                            else:
                                raise

                    val_progress.set_postfix({'val_loss': loss.item()})

            avg_val_loss = total_val_loss / len(val_dataloader)
            logger.info(f"éªŒè¯æŸå¤±: {avg_val_loss:.4f}")

            # è®¡ç®—å‡†ç¡®ç‡ - å…¼å®¹æ€§ä¿®å¤
            val_accuracies = {}
            for task in ['standard', 'level1', 'level2', 'level3']:
                # è½¬æ¢ä¸ºnumpyæ•°ç»„å…¼å®¹æ ¼å¼
                try:
                    labels_array = np.array(all_labels[task])
                    preds_array = np.array(all_predictions[task])
                    accuracy = accuracy_score(labels_array, preds_array)
                except (RuntimeError, ImportError):
                    # å¦‚æœnumpyä¸å¯ç”¨ï¼Œæ‰‹åŠ¨è®¡ç®—å‡†ç¡®ç‡
                    correct = sum(1 for l, p in zip(all_labels[task], all_predictions[task]) if l == p)
                    accuracy = correct / len(all_labels[task]) if all_labels[task] else 0

                val_accuracies[task] = accuracy
                logger.info(f"{task} å‡†ç¡®ç‡: {accuracy:.4f}")

            # è®°å½•åˆ°WandB - å…¼å®¹æ€§ä¿®å¤
            accuracy_values = list(val_accuracies.values())
            avg_accuracy = sum(accuracy_values) / len(accuracy_values) if accuracy_values else 0

            wandb.log({
                'epoch': epoch + 1,
                'avg_train_loss': avg_train_loss,
                'avg_val_loss': avg_val_loss,
                'val_accuracy_standard': val_accuracies['standard'],
                'val_accuracy_level1': val_accuracies['level1'],
                'val_accuracy_level2': val_accuracies['level2'],
                'val_accuracy_level3': val_accuracies['level3'],
                'val_accuracy_avg': avg_accuracy
            })

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0

                # ç¡®ä¿modelsç›®å½•å­˜åœ¨
                models_dir = './models'
                os.makedirs(models_dir, exist_ok=True)

                # ä¿å­˜æ¨¡å‹
                model_path = os.path.join(models_dir, 'best_model')
                tokenizer_path = os.path.join(models_dir, 'tokenizer')
                mappings_path = os.path.join(models_dir, 'label_mappings.json')

                logger.info(f"ä¿å­˜æ¨¡å‹åˆ°: {model_path}")
                model.save_pretrained(model_path)

                logger.info(f"ä¿å­˜åˆ†è¯å™¨åˆ°: {tokenizer_path}")
                tokenizer.save_pretrained(tokenizer_path)

                # ä¿å­˜æ ‡ç­¾æ˜ å°„
                label_mappings = {
                    'standard_name': train_dataset.standard_mapping,
                    'level1_category': train_dataset.level1_mapping,
                    'level2_category': train_dataset.level2_mapping,
                    'level3_category': train_dataset.level3_mapping
                }

                with open(mappings_path, 'w', encoding='utf-8') as f:
                    json.dump(label_mappings, f, ensure_ascii=False, indent=2)

                logger.info(f"ä¿å­˜æ ‡ç­¾æ˜ å°„åˆ°: {mappings_path}")

                logger.info("âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                patience_counter += 1
                logger.info(f"éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{max_patience}")

                if patience_counter >= max_patience:
                    logger.info("æ—©åœè®­ç»ƒ")
                    break

        # è®­ç»ƒå®Œæˆ
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        wandb.finish()

        return True

    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    try:
        args = parse_arguments()
    except SystemExit:
        # å‚æ•°è§£æå¤±è´¥ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        return False

    print("äº§å“åˆ†ç±»æ¨¡å‹ - ModelScopeæœ€ç»ˆç‰ˆ")
    print("=" * 60)

    # æ˜¾ç¤ºModelScopeä¿¡æ¯
    print("ModelScope é…ç½®ä¿¡æ¯:")
    print("  - æ¨¡å‹æº: ModelScope (é­”æ­)")
    print("  - æ¨¡å‹åç§°: dienstag/chinese-bert-wwm-ext")
    print("  - ä¸‹è½½å¹³å°: å›½å†…é«˜é€ŸæœåŠ¡å™¨")
    print("  - æ— HuggingFaceä¾èµ–: OK")
    print("  - è‡ªåŠ¨ç¼“å­˜: OK")
    print("  - ç¦»çº¿è¿è¡Œ: OK")

    # æ•°æ®æ–‡ä»¶æ£€æŸ¥
    if not check_data_files(args.train_path, args.val_path):
        logger.error("æ•°æ®æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
        return False

    # å¼€å§‹è®­ç»ƒ
    success = run_training()

    if success:
        print("\nè®­ç»ƒå®Œæˆ!")
        print("è¾“å‡ºæ–‡ä»¶:")
        print("  - ./models/best_model/: æœ€ä½³æ¨¡å‹æ–‡ä»¶")
        print("  - ./models/tokenizer/: åˆ†è¯å™¨æ–‡ä»¶")
        print("  - ./models/label_mappings.json: æ ‡ç­¾æ˜ å°„æ–‡ä»¶")
        print("  - ./models/config.json: æ¨¡å‹é…ç½®æ–‡ä»¶")
        print("  - ./models/pytorch_model.bin: æ¨¡å‹æƒé‡æ–‡ä»¶")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. ä¸‹è½½ models/ ç›®å½•åˆ°æœ¬åœ°")
        print("  2. è¿è¡Œ python run_inference.py æµ‹è¯•æ¨ç†")
        print("  3. è¿è¡Œ python deploy_app.py å¯åŠ¨WebæœåŠ¡")
        print("\nç›‘æ§å’Œæ—¥å¿—:")
        print("  - WandBé¡¹ç›®é¡µé¢æŸ¥çœ‹è¯¦ç»†æŒ‡æ ‡")
        print("  - æœ¬åœ°æ—¥å¿—: tail -f logs/training.log")
    else:
        print("\nè®­ç»ƒå¤±è´¥")
        print("è¯·æ£€æŸ¥:")
        print("  1. æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("  2. Pythonç¯å¢ƒå’Œä¾èµ–åŒ…")
        print("  3. ç½‘ç»œè¿æ¥çŠ¶æ€")
        print("  4. GPUèµ„æºæ˜¯å¦å¯ç”¨")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)