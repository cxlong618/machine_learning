# 无监督学习详解

## 🎯 无监督学习概述

无监督学习是机器学习的重要分支，它从没有标签的数据中发现隐藏的模式、结构和关系。与监督学习不同，无监督学习不需要预先标记的目标值。

## 📊 无监督学习的主要类型

### 1. 聚类 (Clustering)

**定义**：将相似的数据点分组到一起

**目标**：组内相似度高，组间相似度低

### 2. 降维 (Dimensionality Reduction)

**定义**：减少数据的特征数量，同时保留重要信息

### 3. 关联规则学习 (Association Rule Learning)

**定义**：发现变量之间的关系

### 4. 异常检测 (Anomaly Detection)

**定义**：识别与其他数据显著不同的数据点

## 🛠️ 常用无监督学习算法

### 1. K-均值聚类 (K-Means)

**原理**：
1. 随机选择K个质心
2. 将每个点分配给最近的质心
3. 重新计算质心位置
4. 重复步骤2-3直到收敛

**优点**：
- 简单快速
- 可扩展性好
- 适用于大规模数据

**缺点**：
- 需要预先指定K值
- 对初始质心敏感
- 只能发现球形簇

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 创建K-Means模型
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# 可视化结果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
           marker='x', s=300, c='red')
plt.show()
```

### 2. 层次聚类 (Hierarchical Clustering)

**原理**：创建数据的层次结构

**类型**：
- **凝聚型**：从单个点开始，逐步合并
- **分裂型**：从所有点开始，逐步分裂

**优点**：
- 不需要预设聚类数量
- 可视化层次结构
- 适合小数据集

**缺点**：
- 计算复杂度高 O(n²logn)
- 对噪声敏感

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram

# 层次聚类
agg_clustering = AgglomerativeClustering(n_clusters=3)
labels = agg_clustering.fit_predict(X)
```

### 3. DBSCAN (Density-Based Spatial Clustering)

**原理**：基于密度的聚类方法

**参数**：
- **eps**：邻域半径
- **min_samples**：形成核心点所需的最小样本数

**优点**：
- 不需要预设聚类数量
- 可以发现任意形状的簇
- 能识别噪声点

**缺点**：
- 对参数敏感
- 难以处理密度不均的数据

```python
from sklearn.cluster import DBSCAN

# DBSCAN聚类
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)
```

### 4. 主成分分析 (PCA)

**原理**：找到数据方差最大的方向

**目标**：用较少的变量表示原始数据的主要信息

**应用**：
- 数据可视化
- 特征提取
- 噪声去除
- 数据压缩

```python
from sklearn.decomposition import PCA

# 降维到2D
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# 查看解释方差比例
print("解释方差比例:", pca.explained_variance_ratio_)
print("累计解释方差比例:", pca.explained_variance_ratio_.sum())
```

### 5. t-SNE (t-Distributed Stochastic Neighbor Embedding)

**原理**：非线性降维技术

**特点**：
- 适用于高维数据可视化
- 保持局部结构
- 计算复杂度较高

```python
from sklearn.manifold import TSNE

# t-SNE降维
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)
```

### 6. 关联规则学习

**Apriori算法**：
- 频繁项集挖掘
- 关联规则发现

**核心概念**：
- **支持度**：项集出现的频率
- **置信度**：规则的可靠性
- **提升度**：规则的相关性

```python
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

# 挖掘频繁项集
frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)

# 生成关联规则
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
```

## 📈 评估指标

### 聚类评估

**轮廓系数 (Silhouette Score)**：
```
(b - a) / max(a, b)
```
- a：样本与同一簇中其他样本的平均距离
- b：样本与最近簇中所有样本的平均距离
- 范围：[-1, 1]，越接近1表示聚类效果越好

**Calinski-Harabasz指数**：
- 簇间离散度与簇内离散度的比值
- 越大越好

**Davies-Bouldin指数**：
- 簇内距离与簇间距离的比值
- 越小越好

### 降维评估

**解释方差比例**：
- 降维后保留的信息量
- 累计解释方差

**重构误差**：
- 降维后再重构的误差
- 越小越好

## 🔄 实际应用案例

### 1. 客户分群

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import seaborn as sns

# 加载客户数据
data = pd.read_csv('customer_data.csv')

# 数据预处理
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[['age', 'income', 'spending_score']])

# K-Means聚类
kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# 添加聚类标签
data['cluster'] = clusters

# 分析每个簇的特征
for i in range(4):
    cluster_data = data[data['cluster'] == i]
    print(f"簇 {i}: 平均年龄={cluster_data['age'].mean():.1f}, "
          f"平均收入={cluster_data['income'].mean():.1f}")
```

### 2. 异常检测

```python
from sklearn.ensemble import IsolationForest

# 异常检测
iso_forest = IsolationForest(contamination=0.1, random_state=42)
anomalies = iso_forest.fit_predict(X)

# -1表示异常点，1表示正常点
anomaly_indices = np.where(anomalies == -1)[0]
print(f"发现 {len(anomaly_indices)} 个异常点")
```

### 3. 推荐系统

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 用户-物品矩阵
user_item_matrix = np.array([[5, 3, 0, 1],
                             [4, 0, 0, 1],
                             [1, 1, 0, 5],
                             [1, 0, 0, 4],
                             [0, 1, 5, 4]])

# 计算用户相似度
user_similarity = cosine_similarity(user_item_matrix)

# 推荐物品
def recommend_items(user_id, user_similarity, user_item_matrix, n_recommendations=3):
    similar_users = np.argsort(user_similarity[user_id])[::-1][1:4]
    recommendations = []

    for item_id in range(user_item_matrix.shape[1]):
        if user_item_matrix[user_id, item_id] == 0:  # 用户未评分的物品
            weighted_rating = 0
            similarity_sum = 0

            for similar_user in similar_users:
                if user_item_matrix[similar_user, item_id] > 0:
                    weighted_rating += user_similarity[user_id, similar_user] * user_item_matrix[similar_user, item_id]
                    similarity_sum += user_similarity[user_id, similar_user]

            if similarity_sum > 0:
                recommendations.append((item_id, weighted_rating / similarity_sum))

    recommendations.sort(key=lambda x: x[1], reverse=True)
    return recommendations[:n_recommendations]
```

## ⚠️ 挑战和解决方案

### 1. 选择合适的算法

**考虑因素**：
- 数据规模
- 数据维度
- 簇的形状
- 噪声水平

### 2. 参数调优

**K值选择**：
- 肘部法则 (Elbow Method)
- 轮廓系数分析
- 跨验证

**密度参数**：
- 基于领域知识
- 实验验证

### 3. 可解释性

**方法**：
- 可视化聚类结果
- 分析簇的特征
- 业务解释

### 4. 计算效率

**优化策略**：
- 采样技术
- 近似算法
- 并行计算
- 增量学习

## 🚀 最佳实践

### 1. 数据预处理
- 标准化/归一化
- 处理缺失值
- 特征选择

### 2. 算法选择流程
```
1. 了解数据特点
2. 明确业务目标
3. 尝试简单算法
4. 逐步增加复杂度
5. 评估和比较
```

### 3. 验证方法
- 内部评估指标
- 外部验证（如果有标签）
- 稳定性检验

### 4. 业务应用
- 结合领域知识
- 可解释性优先
- 持续监控和更新

---

*无监督学习是数据探索和知识发现的重要工具，在实际业务中有着广泛的应用价值！*